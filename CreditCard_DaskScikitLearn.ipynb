{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "from dask.distributed import Client, progress\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:52331</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:52330/status' target='_blank'>http://127.0.0.1:52330/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>7.45 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:52331' processes=4 threads=8, memory=7.45 GiB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('creditcard.csv', dtype={'Time': 'float64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. use all of the variables except Class as your feature set. The Class variable will be your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=3\n",
       "    int64\n",
       "      ...\n",
       "      ...\n",
       "      ...\n",
       "Name: Class, dtype: int64\n",
       "Dask Name: split, 3 tasks"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our feature set\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "\n",
    "# This is our target variable\n",
    "Y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Since our data can fit into memory\n",
    "# we persist them to the RAM.\n",
    "X_train.persist()\n",
    "X_test.persist()\n",
    "y_train.persist()\n",
    "y_test.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8505188745838435\n",
      "test score: 0.8491808043342269\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    lr.fit(X_train.compute(), y_train.compute())\n",
    "    \n",
    "preds_train = lr.predict(X_train.values.compute())\n",
    "preds_test = lr.predict(X_test.values.compute())\n",
    "\n",
    "print(f'logistic regression train score:', roc_auc_score(preds_train, y_train.values.compute()))\n",
    "print(f'logistic regression test score:', roc_auc_score(preds_test, y_test.values.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest training score:  1.0\n",
      "random forest test score:  0.9873328612396417\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    rfc.fit(X_train.compute(), y_train.compute())\n",
    "    \n",
    "preds_train = rfc.predict(X_train.values.compute())\n",
    "preds_test = rfc.predict(X_test.values.compute())\n",
    "\n",
    "print(\"random forest training score: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
    "print(\"random forest test score: \", roc_auc_score(preds_test, y_test.values.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient boosting tree training score:  0.9211210837960387\n",
      "gradient boosting tree test score:  0.898435092063888\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    gbc.fit(X_train.compute(), y_train.compute())\n",
    "    \n",
    "preds_train = gbc.predict(X_train.values.compute())\n",
    "preds_test = gbc.predict(X_test.values.compute())\n",
    "\n",
    "print(\"gradient boosting tree training score: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
    "print(\"gradient boosting tree test score: \", roc_auc_score(preds_test, y_test.values.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare the results of your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest performed the best with a 98.7% accuracy in the test score.\n",
    "\n",
    "Logistic Regression performed the worst with an 84.5% accuracy\n",
    "\n",
    "Gradient Boosting did a decent job with an 89.8% accuracy, however, it overfit worse.\n",
    "\n",
    "However, notice that the class imbalance is huge between fraud and non-fraud transactions leading to a high accuracy score<br>\n",
    "Using class imbalance techniques such as SMOTE could help with this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
